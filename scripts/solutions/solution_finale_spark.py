# Solution finale pour Spark sur EMR avec JupyterHub
# ExÃ©cutez ce code dans une NOUVELLE cellule aprÃ¨s avoir installÃ© findspark

print("ğŸ”§ Configuration Spark avec findspark...")
print("=" * 70)

# 1. Installer findspark (si pas dÃ©jÃ  fait)
import subprocess
import sys

try:
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'findspark', '--user'], 
                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    print("âœ… findspark installÃ©")
except:
    print("â„¹ï¸  findspark dÃ©jÃ  installÃ© ou installation en cours...")

# 2. Recharger sys.path pour trouver findspark
import importlib
import site
site.addsitedir(f"{os.environ.get('HOME', '/home/hadoop')}/.local/lib/python3.9/site-packages")

# 3. Importer findspark
try:
    import findspark
    print("âœ… findspark importÃ©")
except ImportError:
    # Essayer de recharger
    importlib.invalidate_caches()
    try:
        import findspark
        print("âœ… findspark importÃ© (aprÃ¨s rechargement)")
    except ImportError:
        print("âŒ findspark non trouvÃ©")
        print("ğŸ’¡ RedÃ©marrez le kernel Python et rÃ©essayez")
        raise

# 4. Initialiser findspark
print("\nğŸš€ Initialisation de findspark...")
try:
    # Essayer la recherche automatique
    findspark.init()
    print("âœ… findspark initialisÃ© (recherche automatique)")
except Exception as e:
    print(f"âš ï¸  Recherche automatique Ã©chouÃ©e: {e}")
    # Essayer avec chemin explicite
    spark_paths = ['/usr/lib/spark', '/opt/spark']
    for spark_path in spark_paths:
        try:
            findspark.init(spark_path)
            print(f"âœ… findspark initialisÃ© avec: {spark_path}")
            break
        except:
            continue
    else:
        print("âŒ Impossible d'initialiser findspark")
        raise

# 5. VÃ©rifier la configuration
import os
print(f"\nğŸ“‹ Configuration:")
print(f"JAVA_HOME: {os.environ.get('JAVA_HOME', 'Non dÃ©fini')}")
print(f"SPARK_HOME: {os.environ.get('SPARK_HOME', 'Non dÃ©fini')}")

# 6. CrÃ©er SparkSession
print("\nğŸš€ CrÃ©ation de SparkSession...")
print("-" * 70)

from pyspark.sql import SparkSession

spark = (SparkSession
         .builder
         .appName('P8')
         .config("spark.sql.parquet.writeLegacyFormat", 'true')
         .getOrCreate()
)

print("âœ… SparkSession crÃ©Ã©e avec succÃ¨s!")
print(f"   Spark version: {spark.version}")
print(f"   Spark master: {spark.sparkContext.master}")

print("\n" + "=" * 70)
print("âœ… Configuration terminÃ©e!")
